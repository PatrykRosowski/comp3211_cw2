import logging, os, pyodbc
import azure.functions as func
from random import randint as randint


app = func.FunctionApp()

# The function used for simulating data collection from sensors
# and performing an insert operation in the database
# It runs every 5 seconds, for testing and demonstration purpose
@app.function_name(name="data_collection")
@app.schedule(schedule="*/5 * * * * *", arg_name="myTimer", 
              run_on_startup=True,
              use_monitor=False)
def data_collection(myTimer: func.TimerRequest) -> None:
    if myTimer.past_due:
        logging.info('The timer is past due!')

    logging.info('data_collection function executed.')

    # The variable holding the number of sensors we are collecting data from
    num_of_sensors = 20

    # A nested dictionary storing data from each sensor
    # key = sensor ID
    # value = a sub-dictionary with the data for this sensor
    collected_data = {}

    # This 'for' loop generates random data for each sensor ID
    # This data is added into 'collected_data' to create a nested dictionary
    for sensor_ID in range(1, num_of_sensors + 1):
        collected_data[sensor_ID] = {
            "Temperature": randint(8, 15),
            "Wind": randint(15, 25),
            "R_Humidity": randint(40, 70),
            "CO2": randint(500, 1500)
        }

    # The sql query used to insert data into the table
    query = """
            INSERT INTO LeedsSensorsData (Sensor_ID, Temperature, Wind, R_Humidity, CO2)
            VALUES (?, ?, ?, ?, ?)
    """

    # To ensure that the ACID rules are followed, I will use the try/except statements
    # and rollback any changes in case of an error
    try:
        # Establish a database connection using the connection string stored in
        # local.settings.json (local) and an environment variable on Azure (deployed)
        db_connection = pyodbc.connect(os.environ["SqlConnectionStringTask1"])
        # Create a cursor object to manipulate the db table using sql queries
        cursor = db_connection.cursor()

        # To improve efficiency I will make sure that the changes are only committed
        # after the program has executed the sql query for each sensor in the dictionary
        db_connection.autocommit = False

        for sensor_ID, data in collected_data.items():
            # Data that will be included in each record
            # Date/time and record ID are automatically generated by the database
            sensor_info = (
                sensor_ID,
                data["Temperature"],
                data["Wind"],
                data["R_Humidity"],
                data["CO2"]
            )

            # Use the cursor to execute the insert operation
            cursor.execute(query, sensor_info)

        # Commit the db changes
        db_connection.commit()

    # If a pyodbc error occurs, roll back the changes and display the error
    except pyodbc.Error as error:
        db_connection.rollback()
        logging.error(error)

    # Set the autocommit attribute back to True, its default value, for clean up
    db_connection.autocommit = True
    # Close the cursor object and the db connection
    cursor.close()
    db_connection.close()

# The function responsible for calculating statistical data
# It gets triggered whenever a change in the database is detected
@app.function_name(name="stats")
@app.generic_trigger(arg_name="change", type="sqlTrigger",
                        TableName="LeedsSensorsData",
                        ConnectionStringSetting="SqlConnectionStringTask2")
def stats(change: str) -> None:

    # SQL query that calculates min, max and avg for each field of each sensor
    # and returns records containing the information that we need to display.
    # The average values are cast to float and rounded to two decimal places, 
    # to avoid situations where the average between 1 and 2 is given as 1 instead of 1.5
    # (AVG statement seems to round down the halves instead of round up)
    query = """
    SELECT 
        Sensor_ID,
        ROUND(AVG(CAST(Temperature AS float)), 2) AS avg_temp,
        MIN(Temperature) AS min_temp,
        MAX(Temperature) AS max_temp,
        ROUND(AVG(CAST(Wind AS float)), 2) as avg_wind,
        MIN(Wind) as min_wind,
        MAX(Wind) as max_wind,
        ROUND(AVG(CAST(R_Humidity AS float)), 2) as avg_humidity,
        MIN(R_Humidity) as min_humidity,
        MAX(R_Humidity) as max_humidity,
        ROUND(AVG(CAST(CO2 AS float)), 2) as avg_co2,
        MIN(CO2) as min_co2,
        MAX(CO2) as max_co2
    FROM 
        LeedsSensorsData
    GROUP BY 
        Sensor_ID;
    """

    try:
        # Establish a database connection using the connection string stored in
        # local.settings.json (local) and an environment variable on Azure (deployed)
        db_connection = pyodbc.connect(os.environ["SqlConnectionStringTask1"])

        # Create a cursor object to manipulate the db table using sql queries
        cursor = db_connection.cursor()

        cursor.execute(query)

        # The logging.info() function below is asynchronous. 
        # I will use the 'sorted_data' list to sort the rows numerically
        # according to the Sensor_ID, and thus improve the readability of results.
        # The print() function could be used instead, locally, but it does not
        # generate output in the 'Logs' window on the Azure platform.
        sorted_data = []

        # Append each retrieved row to the 'sorted_data' list
        for record in cursor:
            sorted_data.append(record)

        # Sort the rows numerically, starting with the smallest Sensor_ID value.
        # The argument passed to the 'key' parameter must be an output from a function.
        # I am using a lambda function to avoid using nested functions which affect code readability.
        sorted_data.sort(key = lambda record: record.Sensor_ID)

        # This string will be used to concatenate ordered data for all the sensors.
        # This is done to avoid logs being shown out of order by the logging.info() function.
        ordered_logs = ""   

        # Concatenate the data for each string with the 'ordered_logs' string
        for record in sorted_data:
            ordered_logs += (f"\nSensor ID: {record.Sensor_ID}, " + 
                            f"Average temp: {record.avg_temp}, " +
                            f"Min. temp: {record.min_temp}, " +
                            f"Max. temp: {record.max_temp}\n" +
                            f"               Average wind: {record.avg_wind}, " +
                            f"Min. wind: {record.min_wind}, " +
                            f"Max. wind: {record.max_wind}\n" +
                            f"               Average humidity: {record.avg_humidity}, " +
                            f"Min. humidity: {record.min_humidity}, " +
                            f"Max. humidity: {record.max_humidity}\n" +
                            f"               Average CO2: {record.avg_co2}, " +
                            f"Min. CO2: {record.min_co2}, " +
                            f"Max. CO2: {record.max_co2}\n")

        # Output the results
        logging.info(ordered_logs)

    # If a pyodbc error occurs, roll back the changes and display the error
    except pyodbc.Error as error:
        db_connection.rollback()
        logging.error(error)

    # Close the cursor object and the db connection
    cursor.close()
    db_connection.close()
